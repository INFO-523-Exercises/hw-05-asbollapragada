---
title: "HW 05"
author: "Anjani Sowmya Bollapragada - 23851219"
format:
  html:
    embed-resources: true
toc: true
---

## 1 - Data selection and exploration


```{r, warning=FALSE, message=FALSE}
#| label: Installing and Loading the packages

# Required package for quick package downloading and loading 
if (!require(pacman))
  install.packages("pacman")

library(pacman)

p_load(dlookr,
       DMwR2, # Data Mining with R functions
       tidyverse, # Data wrangling, manipulation, visualization
       DBI, # DBI databases
       formattable, # HTML tables from R outputs
               here, # Standardizes paths to data
               kableExtra, # Alternative to formattable
               knitr, # Needed to write HTML reports
               missRanger, # To generate NAs
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       qqplotr) 
```

```{r, warning=FALSE}
#| label: Loading-the-csv-file

haunted_places <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-10/haunted_places.csv', show_col_types = FALSE)
```

### Describing all columns

```{r}
#| label: Describing the columns

# Create a sample data frame to describe the data
my_data <- data.frame(
  variable = c("city","country","description","location","state","state_abbrev","longitude","latitude","city_longitude","city_latitude"),
  class = c("character","character","character","character","character","character","double","double","double","double"),
  description = c("The city where the place is located.",
"The country where the place is located - only United States",
"A text description of the place.",
"A title for the haunted place.",
"The US state where the place is located.",
"The two-letter abbreviation for the state.",
"Longitude of the place.",
"Latitude of the place.",
"Longitude of the city center.",
"Latitude of the city center.")
)

library(knitr)

# Print the data frame as a table using knitr::kable()
kable(my_data, caption = "Description of the columns", align = "c")

```

### Describing and interpreting missing values and outliers

```{r}
#| label: Finding missing values

library(purrr)
# Compute the total number of NA values in the dataset
nas <- haunted_places %>% 
  purrr::map_dbl(~sum(is.na(.))) %>% 
  sum()

cat("The dataset contains ", nas, "NA values. \n")
```

```{r, warning=FALSE}
#| label: Finding incomplete rows

# Compute the number of incomplete rows in the dataset
incomplete_rows <- haunted_places %>% 
  summarise_all(~!complete.cases(.)) %>%
  nrow()

cat("The dataset contains ", incomplete_rows, "(out of ", nrow(haunted_places),") incomplete rows. \n")
```

```{r}
#| label: Finding the Outliers (Boxplot)

# Selecting numeric columns for outlier detection (longitude and latitude)
numeric_columns <- c("longitude", "latitude")

# Filtering out non-finite values from the selected columns
valid_data <- haunted_places[rowSums(is.na(haunted_places[, numeric_columns])) == 0, ]

# Creating boxplots for numeric columns to identify outliers
par(mfrow = c(1, length(numeric_columns)))  # Setting the layout for multiple plots

for (col in numeric_columns) {
  boxplot(valid_data[[col]], main = col, ylab = col, outline = TRUE)
}

```

```{r, warning=FALSE}
#| label: Summary of Outliers

# Selecting numeric columns for outlier detection (longitude and latitude)
numeric_columns <- c("longitude", "latitude")

# Function to convert to numeric and calculate outlier metrics
calculate_outlier_metrics <- function(data, column) {
  # Convert the column to numeric, handling non-convertible values
  data[[column]] <- as.numeric(as.character(data[[column]]))
  
  # Filter out non-numeric values (NA after conversion)
  numeric_data <- data[!is.na(data[[column]]), ]
  
  # Calculating quartiles for outlier detection
  Q1 <- quantile(numeric_data[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(numeric_data[[column]], 0.75, na.rm = TRUE)
  
  # Interquartile range (IQR)
  IQR <- Q3 - Q1
  
  # Outlier boundaries
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify outliers
  outliers <- numeric_data[numeric_data[[column]] < lower_bound | numeric_data[[column]] > upper_bound, column]
  
  # Calculate metrics
  outliers_cnt <- length(outliers)
  outliers_ratio <- outliers_cnt / length(numeric_data[[column]])
  
  # Return metrics
  return(list(outliers_cnt = outliers_cnt, 
              outliers_ratio = outliers_ratio))
}

# Calculate metrics for each numeric column
outlier_metrics <- lapply(numeric_columns, function(col) {
  calculate_outlier_metrics(haunted_places, col)
})

# Convert the list of metrics to a data frame
outlier_metrics_df <- do.call(rbind.data.frame, outlier_metrics)
rownames(outlier_metrics_df) <- numeric_columns


outlier_metrics_df
```

### Identifying the relationships for examination:

Given the nature of the data containing information about haunted places, the variables available are mostly categorical (character) or geographical (latitude, longitude). All the haunted places are located only in the United States. Also, the amount of detail in descriptions of these locations is highly variable. Therefore, the relationships to examine in this context could involve investigating whether there's a relationship between the geographical coordinates (latitude and longitude) of haunted places and their corresponding city, state, or country.

### Question to solve

Can we predict the latitude or longitude of a haunted place based on its location description (i.e., city, state, country)?

For this question, creating a regression model where latitude or longitude is the dependent variable, and the independent variables include categorical variables like city, state, and country might be useful. This could help explore whether certain locations tend to have specific latitude or longitude coordinates associated with haunted places.

## 2 - Data preprocessing

### Data Cleaning - Handling Missing Values and Outliers

As observed from above conclusions about missing values and outliers, since there is only one outlier, that need not be handled. But there are 2586 NA values in the dataset which needs to be handled

```{r}
#| label: Removing columns with many NAs
library(dplyr)

missing.value.rows <- haunted_places |>
  filter(!complete.cases(haunted_places))
missing.value.rows

```

1272 out of 10992 rows contains an NA

```{r}
#| label: Finding the number of NAs in each row

haunted_places <- haunted_places %>%
  mutate(na_count = rowSums(is.na(haunted_places)))
# Find the maximum count of NA values
max_na_count <- max(haunted_places$na_count)
# Print rows with the maximum number of NA values
rows_with_max_na <- haunted_places[haunted_places$na_count == max_na_count, ]
print(rows_with_max_na)

```

There are only three rows with maximum NA values. The easiest way to handle them is to remove the three columns.

```{r}
#| label: Removing rows with maximum NA values.

# Remove rows with the maximum number of NA values from the dataset
cleaned_haunted_places <- haunted_places[haunted_places$na_count != max_na_count, ]

# Remove the 'na_count' column if you no longer need it
haunted_places <- select(cleaned_haunted_places, -na_count)
```

```{r}
#| label: Finding a random row with NAs

haunted_places[7,]
```

Here, the latitude and longitude have NAs. Aiming for mean or median imputation for the latitude first can be a good approach.

```{r, warning=FALSE}
#| label:  plot a QQ plot of latitude
install.packages("car")
library(car)
ggplot(haunted_places, aes(sample = latitude)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of latitude") 
```

Since the straight line fits the data pretty well so latitude is normal, mean value can be filled in the unknown.

```{r}
#| label: Mean Imputation for the selected row.
haunted_places <- haunted_places |>
  mutate(latitude = ifelse(row_number() == 7, mean(latitude, na.rm = TRUE), latitude))
haunted_places
```

Now, it's time for longitude.

```{r, warning=FALSE}
#| label:  plot a QQ plot of longitude
ggplot(haunted_places, aes(sample = longitude)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of latitude") 
```

```{r}
#| label: Finding mean and median for longitude
median(haunted_places$longitude, na.rm = TRUE)
mean(haunted_places$longitude, na.rm = TRUE)
```

Median seems to be more appropriate value to replace NAs in longitude and NAs in latitude with mean should be a optimal solution. 

```{r}
#| label: Mean Imputation for Latitude
haunted_places <- haunted_places |>
  mutate(latitude = if_else(is.na(latitude), mean(latitude, na.rm = TRUE), latitude))
```

```{r}
#| label: Median Imputation for Longitude
haunted_places <- haunted_places |>
  mutate(longitude = if_else(is.na(longitude), median(longitude, na.rm = TRUE), longitude))

haunted_places
```

```{r}
#| label: Finding remaining NAs
missing.value.rows <- haunted_places |>
  filter(!complete.cases(haunted_places))
missing.value.rows
```

Still, there are 29 rows out of 10989 rows which can be removed.

```{r}
#| label: Removing the remaining 29 rows 

haunted_places <- haunted_places %>%
  filter(complete.cases(.))
```


## 3 - Ordinary Least Squares Regression


## 4 - Alternative regressions 

## 5 - Model performance and correct selection

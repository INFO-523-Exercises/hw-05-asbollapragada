---
title: "HW 05"
author: "Anjani Sowmya Bollapragada - 23851219"
format:
  html:
    embed-resources: true
toc: true
---

## 1 - Data selection and exploration


```{r, warning=FALSE, message=FALSE}
#| label: Installing and Loading the packages

# Required package for quick package downloading and loading 
if (!require(pacman))
  install.packages("pacman")

library(pacman)

p_load(dlookr,
       DMwR2, # Data Mining with R functions
       tidyverse, # Data wrangling, manipulation, visualization
       DBI, # DBI databases
       formattable, # HTML tables from R outputs
               here, # Standardizes paths to data
               kableExtra, # Alternative to formattable
               knitr, # Needed to write HTML reports
               missRanger, # To generate NAs
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       qqplotr) 
```

```{r, warning=FALSE}
#| label: Loading-the-csv-file

haunted_places <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-10/haunted_places.csv', show_col_types = FALSE)
```

### Describing all columns

```{r}
#| label: Describing the columns

# Create a sample data frame to describe the data
my_data <- data.frame(
  variable = c("city","country","description","location","state","state_abbrev","longitude","latitude","city_longitude","city_latitude"),
  class = c("character","character","character","character","character","character","double","double","double","double"),
  description = c("The city where the place is located.",
"The country where the place is located - only United States",
"A text description of the place.",
"A title for the haunted place.",
"The US state where the place is located.",
"The two-letter abbreviation for the state.",
"Longitude of the place.",
"Latitude of the place.",
"Longitude of the city center.",
"Latitude of the city center.")
)

library(knitr)

# Print the data frame as a table using knitr::kable()
kable(my_data, caption = "Description of the columns", align = "c")

```

### Describing and interpreting missing values and outliers

```{r}
#| label: Finding missing values

library(purrr)
# Compute the total number of NA values in the dataset
nas <- haunted_places %>% 
  purrr::map_dbl(~sum(is.na(.))) %>% 
  sum()

cat("The dataset contains ", nas, "NA values. \n")
```

```{r, warning=FALSE}
#| label: Finding incomplete rows

# Compute the number of incomplete rows in the dataset
incomplete_rows <- haunted_places %>% 
  summarise_all(~!complete.cases(.)) %>%
  nrow()

cat("The dataset contains ", incomplete_rows, "(out of ", nrow(haunted_places),") incomplete rows. \n")
```

```{r}
#| label: Finding the Outliers (Boxplot)

# Selecting numeric columns for outlier detection (longitude and latitude)
numeric_columns <- c("longitude", "latitude")

# Filtering out non-finite values from the selected columns
valid_data <- haunted_places[rowSums(is.na(haunted_places[, numeric_columns])) == 0, ]

# Creating boxplots for numeric columns to identify outliers
par(mfrow = c(1, length(numeric_columns)))  # Setting the layout for multiple plots

for (col in numeric_columns) {
  boxplot(valid_data[[col]], main = col, ylab = col, outline = TRUE)
}

```

```{r, warning=FALSE}
#| label: Summary of Outliers

# Selecting numeric columns for outlier detection (longitude and latitude)
numeric_columns <- c("longitude", "latitude")

# Function to convert to numeric and calculate outlier metrics
calculate_outlier_metrics <- function(data, column) {
  # Convert the column to numeric, handling non-convertible values
  data[[column]] <- as.numeric(as.character(data[[column]]))
  
  # Filter out non-numeric values (NA after conversion)
  numeric_data <- data[!is.na(data[[column]]), ]
  
  # Calculating quartiles for outlier detection
  Q1 <- quantile(numeric_data[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(numeric_data[[column]], 0.75, na.rm = TRUE)
  
  # Interquartile range (IQR)
  IQR <- Q3 - Q1
  
  # Outlier boundaries
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify outliers
  outliers <- numeric_data[numeric_data[[column]] < lower_bound | numeric_data[[column]] > upper_bound, column]
  
  # Calculate metrics
  outliers_cnt <- length(outliers)
  outliers_ratio <- outliers_cnt / length(numeric_data[[column]])
  
  # Return metrics
  return(list(outliers_cnt = outliers_cnt, 
              outliers_ratio = outliers_ratio))
}

# Calculate metrics for each numeric column
outlier_metrics <- lapply(numeric_columns, function(col) {
  calculate_outlier_metrics(haunted_places, col)
})

# Convert the list of metrics to a data frame
outlier_metrics_df <- do.call(rbind.data.frame, outlier_metrics)
rownames(outlier_metrics_df) <- numeric_columns


outlier_metrics_df
```

### Identifying the relationships for examination:

Given the nature of the data containing information about haunted places, the variables available are mostly categorical (character) or geographical (latitude, longitude). All the haunted places are located only in the United States. Also, the amount of detail in descriptions of these locations is highly variable. Therefore, the relationships to examine in this context could involve investigating whether there's a relationship between the geographical coordinates (latitude and longitude) of haunted places and their corresponding city, state, or country.

### Question to solve

Can we predict the latitude or longitude of a haunted place based on its location description (i.e., city, state, country)?

For this question, creating a regression model where latitude or longitude is the dependent variable, and the independent variables include categorical variables like city, state, and country might be useful. This could help explore whether certain locations tend to have specific latitude or longitude coordinates associated with haunted places.

## 2 - Data preprocessing

### Data Cleaning - Handling Missing Values and Outliers

As observed from above conclusions about missing values and outliers, since there is only one outlier, that need not be handled. But there are 2586 NA values in the dataset which needs to be handled

```{r}
#| label: Removing columns with many NAs
library(dplyr)

missing.value.rows <- haunted_places |>
  filter(!complete.cases(haunted_places))
missing.value.rows

```

1272 out of 10992 rows contains an NA

```{r}
#| label: Finding the number of NAs in each row

haunted_places <- haunted_places %>%
  mutate(na_count = rowSums(is.na(haunted_places)))
# Find the maximum count of NA values
max_na_count <- max(haunted_places$na_count)
# Print rows with the maximum number of NA values
rows_with_max_na <- haunted_places[haunted_places$na_count == max_na_count, ]
print(rows_with_max_na)

```

There are only three rows with maximum NA values. The easiest way to handle them is to remove the three columns.

```{r}
#| label: Removing rows with maximum NA values.

# Remove rows with the maximum number of NA values from the dataset
cleaned_haunted_places <- haunted_places[haunted_places$na_count != max_na_count, ]

# Remove the 'na_count' column if you no longer need it
haunted_places <- select(cleaned_haunted_places, -na_count)
```

```{r}
#| label: Finding a random row with NAs

haunted_places[7,]
```

Here, the latitude and longitude have NAs. Aiming for mean or median imputation for the latitude first can be a good approach.

```{r, warning=FALSE}
#| label:  plot a QQ plot of latitude
install.packages("car")
library(car)
ggplot(haunted_places, aes(sample = latitude)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of latitude") 
```

Since the straight line fits the data pretty well so latitude is normal, mean value can be filled in the unknown.

```{r}
#| label: Mean Imputation for the selected row.
haunted_places <- haunted_places |>
  mutate(latitude = ifelse(row_number() == 7, mean(latitude, na.rm = TRUE), latitude))
haunted_places
```

Now, it's time for longitude.

```{r, warning=FALSE}
#| label:  plot a QQ plot of longitude
ggplot(haunted_places, aes(sample = longitude)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of latitude") 
```

```{r}
#| label: Finding mean and median for longitude
median(haunted_places$longitude, na.rm = TRUE)
mean(haunted_places$longitude, na.rm = TRUE)
```

Median seems to be more appropriate value to replace NAs in longitude and NAs in latitude with mean should be a optimal solution. 

```{r}
#| label: Mean Imputation for Latitude
haunted_places <- haunted_places |>
  mutate(latitude = if_else(is.na(latitude), mean(latitude, na.rm = TRUE), latitude))
```

```{r}
#| label: Median Imputation for Longitude
haunted_places <- haunted_places |>
  mutate(longitude = if_else(is.na(longitude), median(longitude, na.rm = TRUE), longitude))

haunted_places
```

```{r}
#| label: Finding remaining NAs
missing.value.rows <- haunted_places |>
  filter(!complete.cases(haunted_places))
missing.value.rows
```

Still, there are 29 rows out of 10989 rows which can be removed.

```{r}
#| label: Removing the remaining 29 rows 

haunted_places <- haunted_places %>%
  filter(complete.cases(.))
```

### Data Transformation - Feature Scaling

```{r}
#| label: Normalization of numeric columns

# select only numeric columns
haunted_places_numeric <- select(haunted_places, latitude, longitude, city_latitude, city_longitude)

# normalize numeric columns
haunted_places_norm <- scale(haunted_places_numeric)

# convert back to data frame and add species column
haunted.norm <- cbind(as.data.frame(haunted_places_norm))

head(haunted.norm)
summary(haunted.norm)
```

### Data Reduction - Dimentionality Reduction

```{r}
#| label: Principal Component Analysis

# Select numeric columns for PCA
pca_data <- subset(haunted_places, select = c("longitude", "latitude", "city_longitude", "city_latitude"))


# Perform PCA
pca_result <- prcomp(pca_data, scale. = TRUE)

# Summary of PCA
summary(pca_result)

# Accessing principal components and variance explained
pca_result$rotation  # Principal components (loadings)
pca_result$sdev^2 / sum(pca_result$sdev^2)  # Variance explained by each PC
```

PC1 has a standard deviation of approximately 1.4956.
PC2 has a standard deviation of approximately 1.2548.
PC3 and PC4 have smaller standard deviations (0.34391 and 0.26516, respectively).

PC1 explains about 55.92% of the total variance.
PC2 explains about 39.36% of the total variance.
PC3 explains about 2.96% of the total variance.
PC4 explains about 1.76% of the total variance.

Together, PC1 and PC2 account for approximately 95.28% of the total variance, which means they capture most of the variability in the data.

```{r}
#| label: Dimentionality Reduction
haunted_places <- haunted_places %>%
  select(-country, -description, -city_latitude, -city_longitude)  # Exclude 'city_latitude' and 'city_longitude'

head(haunted_places)
```


## 3 - Ordinary Least Squares Regression

Conducting an Ordinary Least Squares (OLS) regression with resampling and evaluating the model performance involves a sequence of steps. 

### Assumption Checks

```{r}

#| label: Assumption Checks


# Load necessary libraries
library(tidymodels)
library(ggplot2)

# Convert 'target_variable' to the integer
haunted_places$target_variable <- as.integer(haunted_places$latitude)  

# Define the target variable and predictor variables
target_variable <- haunted_places$latitude  

# Select predictor variables (excluding non-numeric columns or the target variable)
predictor_variables <- select(haunted_places, -target_variable, -city, -location, -state, -state_abbrev)

# Perform Ordinary Least Squares (OLS) regression
ols_model <- lm(target_variable ~ ., data = predictor_variables)

# Summary of the OLS model
summary(ols_model)

# Check assumptions - Residuals vs Fitted Values (Linearity)
plot(ols_model, which = 1)

# Check assumptions - Normal Q-Q Plot (Normality of Residuals)
plot(ols_model, which = 2)

# Check assumptions - Scale-Location Plot (Homoscedasticity)
plot(ols_model, which = 3)

# Check assumptions - Residuals vs Leverage (Influence)
plot(ols_model, which = 5)

```

Interpretation of the Result:

Residuals:
Min to Max Values: The range of residuals (difference between observed and predicted values) is between -1.097e-13 and 1.140e-11. Residuals are very close to zero, indicating that the model predicts the observed values very accurately.

Coefficients:
Intercept (Interpretation): The intercept of approximately -9.264e-13 suggests that when both longitude and latitude are zero, the predicted value of the target variable is around -9.264e-13.

Longitude (Estimate): The coefficient for longitude is approximately -8.201e-16. However, the p-value of < 2e-16 suggests that longitude is statistically significant in predicting the target variable.

Latitude (Estimate): The coefficient for latitude is approximately 1.000, which means that for every one-unit increase in latitude, the predicted value of the target variable increases by 1.000 units. This predictor is highly statistically significant (p-value < 2e-16).

Model Fit:
R-squared and Adjusted R-squared: The R-squared value of 1 indicates that 100% of the variance in the target variable is explained by the model. The adjusted R-squared (same value) confirms the lack of overfitting.

F-statistic: The extremely high F-statistic (1.132e+31) and its associated p-value (< 2.2e-16) indicate that the overall model is statistically significant, suggesting that at least one predictor significantly contributes to the model.

Residual Standard Error: The residual standard error (RSE) of 1.089e-13 represents the standard deviation of the residuals, measuring the average distance of the observed values from the predicted values. Lower values indicate a better fit of the model to the data.

In summary, both latitude and longitude variables appear to be highly significant in predicting the target variable. The model explains 100% of the variance in the target variable, suggesting an excellent fit to the data.


### Splitting the Dataset

```{r, message=FALSE}
#| label: Splitting the Dataset
# Load necessary libraries
library(caret)


# Set seed for reproducibility
set.seed(123)

# Create indices for the split (80% training, 20% testing)
train_indices <- createDataPartition(haunted_places$target_variable, p = 0.8, list = FALSE)

# Create training and testing datasets based on the indices
train_data <- haunted_places[train_indices, ]
test_data <- haunted_places

```

### Model Building

```{r}
#| label: Model Building

# Convert character variables to factors with specific levels
train_data$city <- factor(train_data$city, levels = unique(train_data$city))
train_data$location <- factor(train_data$location, levels = unique(train_data$location))
train_data$state <- factor(train_data$state, levels = unique(train_data$state))
train_data$state_abbrev <- factor(train_data$state_abbrev, levels = unique(train_data$state_abbrev))

# Define the formula for the OLS regression model
formula <- target_variable ~ . - city - location - state - state_abbrev


# Build the OLS regression model using the training data
ols_model_train <- lm(formula, data = train_data)

# Summary of the OLS model using training data
summary(ols_model_train)


```

Interpretation of the result:

Residuals:
Min to Max Values: The range of residuals (difference between observed and predicted values) is between -0.51537 and 0.87792. Residuals are symmetrically distributed around zero, suggesting that the model's errors have no particular trend.

Coefficients:
Intercept (Interpretation): The intercept of approximately -0.485 implies that when both longitude and latitude are zero, the predicted value of the target variable is around -0.485.

Longitude (Estimate): The coefficient for longitude is approximately -7.898e-05. However, the p-value of 0.66 suggests that longitude is not statistically significant in predicting the target variable at conventional significance levels (e.g., α = 0.05).

Latitude (Estimate): The coefficient for latitude is approximately 0.9998. It indicates that for every one-unit increase in latitude, the predicted value of the target variable increases by about 0.9998 units. This predictor is highly statistically significant (p-value < 2e-16).

Model Fit:
R-squared and Adjusted R-squared: The R-squared value of approximately 0.9968 indicates that around 99.68% of the variance in the target variable is explained by the model, indicating a very good fit. The adjusted R-squared (same value) confirms the lack of overfitting.

F-statistic: The very high F-statistic (1.361e+06) and its associated p-value (< 2.2e-16) indicate that the overall model is statistically significant, suggesting that at least one predictor significantly contributes to the model.

Residual Standard Error: The residual standard error (RSE) of 0.2803 represents the standard deviation of the residuals, measuring the average distance of the observed values from the predicted values. Lower values indicate a better fit of the model to the data.

In summary, the latitude variable appears to be highly significant in predicting the target variable, while longitude does not significantly contribute to the model. The model explains a very high percentage of variance in the target variable and demonstrates an excellent fit to the data.

### Model Diagnostics

```{r}
#| label: Model Diagnostics

# Residuals vs Fitted Values Plot (Linearity Check)
plot(ols_model_train, which = 1)

# Normal Q-Q Plot (Normality of Residuals Check)
plot(ols_model_train, which = 2)

# Scale-Location Plot (Homoscedasticity Check)
plot(ols_model_train, which = 3)

# Residuals vs Leverage Plot (Influence Check)
plot(ols_model_train, which = 5)

# Examine variable significance using p-values
coefficients_table <- tidy(ols_model_train)
coefficients_table

```

Interpretation of the result: 

From the coefficients table, 'Latitude' seems to be a significant predictor in the model as indicated by its low p-value, suggesting it has a strong relationship with the dependent variable. Conversely, 'longitude' does not appear to significantly impact the dependent variable based on its non-significant p-value.

### Evaluate Model Performance

```{r}
#| label: Evaluate Model Performance

# Ensure the 'city' variable levels in test data match those in the training data
test_data$city <- factor(test_data$city, levels = levels(train_data$city))

# Ensure the 'location' variable levels in test data match those in the training data
test_data$location <- factor(test_data$location, levels = levels(train_data$location))

# Apply the model to the modified test set to get predictions
y_pred_test <- predict(ols_model_train, newdata = test_data)

# Create a data frame for plotting
plot_data <- data.frame(True_Values = as.vector(test_data$target_variable), Predicted_Values = y_pred_test)

# Plotting true vs predicted values
ggplot(plot_data, aes(x = True_Values, y = Predicted_Values)) +
  geom_point(color = 'black') +
  ggtitle('Comparing true and predicted values for test set') +
  xlab('True values for y') +
  ylab('Predicted values for y')


```

We can see that the values are more concentrated at the positive values. i.e. the true values and predicted values are most similar starting from 25.

```{r}
#| label: Evaluate Model Accuracy

# Calculate R-squared
rsquared <- cor(test_data$target_variable, y_pred_test)^2

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mean((test_data$target_variable - y_pred_test)^2))

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(test_data$target_variable - y_pred_test))

# Print the calculated metrics
cat("R-squared:", rsquared, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")

```

Interpretation of the results:

R-squared (R²): The R-squared value of approximately 0.997 (or 99.7%) suggests that the model explains around 99.7% of the variance in the target variable. This high value indicates that the model fits the data extremely well.

Root Mean Squared Error (RMSE): The RMSE value of approximately 0.280 indicates that, on average, the model's predictions differ from the actual values by about 0.280 units. Lower RMSE values indicate better accuracy, and this value is relatively low, suggesting good performance.

Mean Absolute Error (MAE): The MAE value of approximately 0.242 implies that, on average, the model's predictions deviate from the actual values by around 0.242 units. Lower MAE values indicate better accuracy, and this value is relatively low, suggesting the model's predictions are close to the actual values.

In summary, these metrics collectively show that the model has a strong predictive capability and performs well in capturing the patterns within the dataset, with high accuracy and minimal errors in predictions.

### Summary of the findings

Process:

Data Preparation:
The dataset consisted of variables such as city, location, longitude, latitude, and a target variable.

Ordinary Least Squares (OLS) regression was performed to predict the target variable using longitude and latitude as predictors.

Model Building:
OLS regression models were built using longitude and latitude as predictors to forecast the target variable.
The model's coefficients were estimated, and the model's statistical significance was evaluated using p-values and F-statistics.

Assumption Checks:
Assumptions of the OLS regression were verified, including linearity, normality of residuals, homoscedasticity, and independence.

Evaluation:
The model's performance was assessed using various metrics such as R-squared, Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE).
Visualization techniques were employed to compare true vs. predicted values.

Findings:

Model Fit: 
The OLS regression model had a perfect fit (R-squared = 1, Adjusted R-squared = 1), indicating that all variance in the target variable was explained by the predictors.
Predictors Significance: Longitude and latitude were significant predictors, as indicated by their p-values (< 2e-16).

Residuals: 
The residuals were exceptionally close to zero, suggesting accurate predictions and a good fit to the observed values.

Performance Metrics: 
R-squared was high (close to 1), and both RMSE and MAE were extremely low, indicating a precise fit of the model to the data.

Conclusion:

The OLS regression model, using longitude and latitude as predictors, demonstrated an exceptional fit to the dataset. Both predictors showed high statistical significance in explaining the variance in the target variable. The model accurately predicted the target variable, as evidenced by the low error metrics (RMSE and MAE) and a perfect fit (R-squared = 1). Overall, the model proved to be a robust and accurate predictor of the target variable based on the provided dataset.

## 4 - Alternative regressions 

### Random Forest Regression

```{r}
#| label: Random Forest Regression

# Load necessary library
library(randomForest)

# Assuming 'haunted_places' contains your dataset

# Define predictor variables (excluding non-numeric columns or the target variable)
predictor_variables <- subset(haunted_places, select = c("longitude", "latitude"))

# Define the target variable
target_variable <- haunted_places$target_variable

# Create training and testing datasets
train_indices <- sample(nrow(haunted_places), 0.8 * nrow(haunted_places))  # 80% train, 20% test
train_data <- predictor_variables[train_indices, ]
test_data <- predictor_variables[-train_indices, ]
train_target <- target_variable[train_indices]
test_target <- target_variable[-train_indices]

# Train the Random Forest model
rf_model <- randomForest(train_target ~ ., data = train_data)

# Predict on test data
predictions <- predict(rf_model, newdata = test_data)

# Evaluate model performance
# Calculate R-squared
rsquared <- cor(test_target, predictions)^2

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mean((test_target - predictions)^2))

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(test_target - predictions))

# Print performance metrics
cat("R-squared:", rsquared, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")



```

Interpretation of the results:

R-squared: 0.9956

R-squared is a measure of how well the model explains the variance in the target variable. In this case, an R-squared value of 0.9956 indicates that approximately 99.56% of the variance in the target variable is explained by the model.

Root Mean Squared Error (RMSE): 0.3184

RMSE measures the average deviation of the predicted values from the actual values. A lower RMSE value signifies better accuracy. Here, the RMSE of 0.3184 indicates that, on average, the model's predictions deviate by approximately 0.3184 units from the actual values.

Mean Absolute Error (MAE): 0.0318

MAE measures the average absolute differences between the predicted and actual values. A lower MAE suggests better model performance. The MAE of 0.0318 implies that, on average, the predictions differ by approximately 0.0318 units from the actual values.

Overall, these metrics suggest that the Random Forest Regression model performs quite well in predicting the target variable based on the provided geographical data, with high accuracy and minimal error.

### Support Vector Regression

```{r, message=FALSE}
#| label: Support Vector Regression (SVR)

# Load necessary library
library(e1071)

# Define predictor variables (excluding non-numeric columns or the target variable)
predictor_variables <- subset(haunted_places, select = c("longitude", "latitude"))

# Define the target variable
target_variable <- haunted_places$target_variable

# Create training and testing datasets
train_indices <- sample(nrow(haunted_places), 0.8 * nrow(haunted_places))  # 80% train, 20% test
train_data <- predictor_variables[train_indices, ]
test_data <- predictor_variables[-train_indices, ]
train_target <- target_variable[train_indices]
test_target <- target_variable[-train_indices]

# Train the Support Vector Regression (SVR) model
svr_model <- svm(train_target ~ ., data = train_data, kernel = "radial")

# Predict on test data
predictions <- predict(svr_model, newdata = test_data)

# Evaluate model performance
# Calculate R-squared
rsquared <- cor(test_target, predictions)^2

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mean((test_target - predictions)^2))

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(test_target - predictions))

# Print performance metrics
cat("R-squared:", rsquared, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")

```

Interpretation of the results:

The Support Vector Regression (SVR) model achieved an R-squared value of 0.9024, indicating that approximately 90.24% of the variance in the target variable is explained by the model.

The Root Mean Squared Error (RMSE) value of 1.5664 suggests that, on average, the model's predictions deviate by approximately 1.5664 units from the actual values.

The Mean Absolute Error (MAE) value of 0.2845 represents the average absolute difference between the predicted and actual values, which is approximately 0.2845 units.

Overall, the SVR model seems to capture a substantial portion of the variance in the target variable with reasonably low errors in prediction.

## 5 - Model performance and correct selection

OLS Model: 
Achieves the highest R-squared value (closest to 1), indicating an excellent fit. It also has the lowest RMSE and MAE among the three models, signifying more accurate predictions and lower errors compared to the other models.

Random Forest Model: 
Demonstrates strong performance with a high R-squared value, indicating a good fit. While having a slightly higher RMSE compared to OLS, it excels in MAE, indicating accurate predictions with very low error on average.

SVR Model: 
Although the SVR model has a respectable R-squared value, it exhibits higher RMSE and MAE compared to the OLS and Random Forest models, suggesting less accurate predictions and higher error in comparison.

### Correct Model Selection:

OLS Regression: 
Best suited when the relationship between predictors and target variable is linear, and when the assumptions of OLS are met. It performs remarkably well in this scenario with high accuracy and low error metrics.

Random Forest Regression: 
Effective for non-linear relationships and handling complex datasets. Although its performance is slightly lower in this case than OLS, it excels in handling non-linear relationships.

SVR (Support Vector Regression): 
While it's a powerful model, it seems less suitable for this dataset as it exhibits lower accuracy and higher error metrics compared to the other models.

Conclusion:

Considering the high R-squared value, low RMSE, and MAE, the *OLS Regression* model appears to be the most suitable choice for this dataset due to its excellent overall performance in accurately predicting the target variable. However, if the relationship is nonlinear or more complex, the Random Forest Regression might be a better choice, as it performs closely to OLS with a lower MAE, indicating precise predictions.
